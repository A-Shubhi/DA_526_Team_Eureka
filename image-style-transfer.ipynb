{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11526816,"sourceType":"datasetVersion","datasetId":7229352},{"sourceId":11526996,"sourceType":"datasetVersion","datasetId":7229490},{"sourceId":11528596,"sourceType":"datasetVersion","datasetId":7230756},{"sourceId":11528617,"sourceType":"datasetVersion","datasetId":7230772},{"sourceId":11529544,"sourceType":"datasetVersion","datasetId":7231463},{"sourceId":11529751,"sourceType":"datasetVersion","datasetId":7231617},{"sourceId":11529824,"sourceType":"datasetVersion","datasetId":7231682},{"sourceId":11533713,"sourceType":"datasetVersion","datasetId":7233924},{"sourceId":11533984,"sourceType":"datasetVersion","datasetId":7234064},{"sourceId":11538168,"sourceType":"datasetVersion","datasetId":7236040},{"sourceId":11539093,"sourceType":"datasetVersion","datasetId":7236528},{"sourceId":11541040,"sourceType":"datasetVersion","datasetId":7237657},{"sourceId":11541233,"sourceType":"datasetVersion","datasetId":7237782}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T19:04:57.725421Z","iopub.execute_input":"2025-04-23T19:04:57.725717Z","iopub.status.idle":"2025-04-23T19:04:57.729680Z","shell.execute_reply.started":"2025-04-23T19:04:57.725696Z","shell.execute_reply":"2025-04-23T19:04:57.729060Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nfrom torch.utils.data import DataLoader\nfrom PIL import Image\nfrom PIL import ImageFilter\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport sys","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TransformerNEt Module\nclass ConvLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super().__init__()\n        padding = kernel_size // 2\n        self.layer = nn.Sequential(\n            nn.ReflectionPad2d(padding),\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n        )\n\n    def forward(self, x):\n        return self.layer(x)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            ConvLayer(channels, channels, 3, 1),\n            nn.InstanceNorm2d(channels, affine=True),\n            nn.ReLU(inplace=True),\n            ConvLayer(channels, channels, 3, 1),\n            nn.InstanceNorm2d(channels, affine=True),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\nclass TransformerNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            ConvLayer(3, 32, 9, 1),\n            nn.InstanceNorm2d(32),\n            nn.ReLU(inplace=True),\n            ConvLayer(32, 64, 3, 2),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n            ConvLayer(64, 128, 3, 2),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(inplace=True)\n        )\n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(128) for _ in range(5)]\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(32),\n            nn.ReLU(inplace=True),\n            ConvLayer(32, 3, 9, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.res_blocks(x)\n        x = self.decoder(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:20:28.182114Z","iopub.execute_input":"2025-04-24T06:20:28.182432Z","iopub.status.idle":"2025-04-24T06:20:28.192352Z","shell.execute_reply.started":"2025-04-24T06:20:28.182411Z","shell.execute_reply":"2025-04-24T06:20:28.191583Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# VGG Feature Extractor\nclass VGGFeatures(nn.Module):\n    def __init__(self, content_layers, style_layers):\n        super().__init__()\n        vgg = models.vgg16(pretrained=True).features[:23]\n        for param in vgg.parameters():\n            param.requires_grad = False\n        self.vgg = vgg.eval()\n        self.content_layers = content_layers\n        self.style_layers = style_layers\n\n    def forward(self, x):\n        features = {}\n        for name, layer in self.vgg._modules.items():\n            x = layer(x)\n            if name in self.content_layers + self.style_layers:\n                features[name] = x\n        return features\n\ndef gram_matrix(feat):\n    B, C, H, W = feat.size()\n    F = feat.view(B, C, H * W)\n    G = torch.bmm(F, F.transpose(1, 2))\n    return G / (C * H * W)\n\ndef total_variation_loss(img):\n    return torch.sum(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:])) + \\\n           torch.sum(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:20:31.745375Z","iopub.execute_input":"2025-04-24T06:20:31.745640Z","iopub.status.idle":"2025-04-24T06:20:31.751879Z","shell.execute_reply.started":"2025-04-24T06:20:31.745620Z","shell.execute_reply":"2025-04-24T06:20:31.751115Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nimage_size = 256\n\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.CenterCrop(image_size),\n    transforms.ToTensor(),  # Converts to [0, 1]\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\n# os.mkdir(\"/kaggle/working/output\")\n# os.mkdir(\"/kaggle/working/output/model4\")\ncontent_dataset = datasets.ImageFolder(\"/kaggle/input/mcoco-1500/mini_coco_sample\", transform=transform)\ncontent_loader = DataLoader(content_dataset, batch_size=4)\n\n# Load style image\n\nstyle_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(256),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\nstyle_img_path = \"/kaggle/input/starry-night-vangogh/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"#\"/kaggle/input/oil-boat-monet/boat.jpg\"\nstyle_image = Image.open(style_img_path).convert(\"L\").convert(\"RGB\").crop((300, 300, 700, 700))#.filter(ImageFilter.GaussianBlur(radius=2))\nstyle_image = style_transform(style_image).unsqueeze(0).to(device)\n\n\n# Extract style features\nvgg = VGGFeatures(content_layers=[\"21\"], style_layers=[\"0\", \"5\", \"10\", \"19\", \"21\"]).to(device)\nstyle_features = vgg(style_image)\n# style_grams = {k: gram_matrix(v) for k, v in style_features.items() if k in vgg.style_layers}\nstyle_grams = {k: gram_matrix(v) for k, v in style_features.items()}\n\n# Initialize model\nmodel = TransformerNet().to(device)\noptimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n\n# Loss weights\nstyle_weight = 1e5\ncontent_weight = 1e0\ntv_weight = 1e-6\nidentity_weight = 1\n\n# Training loop\nfor epoch in range(50): \n    for i, (content_batch, _) in enumerate(content_loader):\n        content_batch = content_batch.to(device)\n        output = model(content_batch)\n\n        output_features = vgg(output)\n        content_features = vgg(content_batch)\n\n        content_loss = torch.nn.functional.mse_loss(output_features[\"21\"], content_features[\"21\"])\n        style_loss = 0\n        for layer in vgg.style_layers:\n            output_gram = gram_matrix(output_features[layer])\n            style_gram = style_grams[layer].expand_as(output_gram)\n            style_loss += torch.nn.functional.mse_loss(output_gram, style_gram)\n\n        tv_loss = total_variation_loss(output)\n        \n        identity = model(content_batch)\n        identity_loss = torch.nn.functional.mse_loss(identity, content_batch)\n\n        loss = content_weight * content_loss + style_weight * style_loss + tv_weight * tv_loss \n        # if epoch > 2:\n            # loss += identity_weight * identity_loss\n            \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if i % 50 == 0:\n            print(f\"Epoch {epoch}, Batch {i}, Loss: {loss.item():.2f}\")\n            # save_image(output[0].cpu().div(255), f\"/kaggle/working/output/model2/output_{epoch}_{i}.jpg\")\n            # show_tensor_image(output[0].unsqueeze(0))\n\n# Save final model\ntorch.save(model.state_dict(), \"output/model4/oil_painting_model14.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:20:58.408254Z","iopub.execute_input":"2025-04-24T06:20:58.408519Z","iopub.status.idle":"2025-04-24T06:30:50.306839Z","shell.execute_reply.started":"2025-04-24T06:20:58.408498Z","shell.execute_reply":"2025-04-24T06:30:50.306266Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 227MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 0, Batch 0, Loss: 33.12\nEpoch 0, Batch 50, Loss: 18.16\nEpoch 0, Batch 100, Loss: 16.24\nEpoch 0, Batch 150, Loss: 12.61\nEpoch 0, Batch 200, Loss: 12.39\nEpoch 0, Batch 250, Loss: 8.10\nEpoch 0, Batch 300, Loss: 7.04\nEpoch 0, Batch 350, Loss: 6.92\nEpoch 1, Batch 0, Loss: 5.52\nEpoch 1, Batch 50, Loss: 7.03\nEpoch 1, Batch 100, Loss: 7.06\nEpoch 1, Batch 150, Loss: 4.85\nEpoch 1, Batch 200, Loss: 6.01\nEpoch 1, Batch 250, Loss: 4.52\nEpoch 1, Batch 300, Loss: 4.34\nEpoch 1, Batch 350, Loss: 5.38\nEpoch 2, Batch 0, Loss: 4.29\nEpoch 2, Batch 50, Loss: 6.19\nEpoch 2, Batch 100, Loss: 6.34\nEpoch 2, Batch 150, Loss: 3.98\nEpoch 2, Batch 200, Loss: 5.47\nEpoch 2, Batch 250, Loss: 4.15\nEpoch 2, Batch 300, Loss: 4.01\nEpoch 2, Batch 350, Loss: 5.04\nEpoch 3, Batch 0, Loss: 3.97\nEpoch 3, Batch 50, Loss: 5.90\nEpoch 3, Batch 100, Loss: 6.05\nEpoch 3, Batch 150, Loss: 3.76\nEpoch 3, Batch 200, Loss: 5.23\nEpoch 3, Batch 250, Loss: 3.97\nEpoch 3, Batch 300, Loss: 3.84\nEpoch 3, Batch 350, Loss: 4.86\nEpoch 4, Batch 0, Loss: 3.80\nEpoch 4, Batch 50, Loss: 5.73\nEpoch 4, Batch 100, Loss: 5.86\nEpoch 4, Batch 150, Loss: 3.61\nEpoch 4, Batch 200, Loss: 5.13\nEpoch 4, Batch 250, Loss: 3.88\nEpoch 4, Batch 300, Loss: 3.73\nEpoch 4, Batch 350, Loss: 4.75\nEpoch 5, Batch 0, Loss: 3.75\nEpoch 5, Batch 50, Loss: 5.57\nEpoch 5, Batch 100, Loss: 5.74\nEpoch 5, Batch 150, Loss: 3.49\nEpoch 5, Batch 200, Loss: 4.98\nEpoch 5, Batch 250, Loss: 3.78\nEpoch 5, Batch 300, Loss: 3.69\nEpoch 5, Batch 350, Loss: 4.64\nEpoch 6, Batch 0, Loss: 3.68\nEpoch 6, Batch 50, Loss: 5.44\nEpoch 6, Batch 100, Loss: 5.62\nEpoch 6, Batch 150, Loss: 3.37\nEpoch 6, Batch 200, Loss: 4.88\nEpoch 6, Batch 250, Loss: 3.69\nEpoch 6, Batch 300, Loss: 3.58\nEpoch 6, Batch 350, Loss: 4.54\nEpoch 7, Batch 0, Loss: 3.64\nEpoch 7, Batch 50, Loss: 5.31\nEpoch 7, Batch 100, Loss: 5.59\nEpoch 7, Batch 150, Loss: 3.30\nEpoch 7, Batch 200, Loss: 4.78\nEpoch 7, Batch 250, Loss: 3.63\nEpoch 7, Batch 300, Loss: 3.48\nEpoch 7, Batch 350, Loss: 4.43\nEpoch 8, Batch 0, Loss: 3.62\nEpoch 8, Batch 50, Loss: 5.24\nEpoch 8, Batch 100, Loss: 5.47\nEpoch 8, Batch 150, Loss: 3.25\nEpoch 8, Batch 200, Loss: 4.70\nEpoch 8, Batch 250, Loss: 3.63\nEpoch 8, Batch 300, Loss: 18.77\nEpoch 8, Batch 350, Loss: 18.49\nEpoch 9, Batch 0, Loss: 17.23\nEpoch 9, Batch 50, Loss: 18.37\nEpoch 9, Batch 100, Loss: 18.19\nEpoch 9, Batch 150, Loss: 15.49\nEpoch 9, Batch 200, Loss: 15.52\nEpoch 9, Batch 250, Loss: 13.46\nEpoch 9, Batch 300, Loss: 12.86\nEpoch 9, Batch 350, Loss: 13.24\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Stylize the Image.\n\ndef load_image(image_path):\n    image = Image.open(image_path).convert(\"RGB\")\n    image = transform(image).unsqueeze(0)  # Add batch dimension\n    return image\n\ndef save_output(tensor, path):\n    image = tensor.clone().detach().cpu().squeeze(0)\n    image = (image + 1) / 2  # From [-1, 1] to [0, 1]\n    image = image.clamp(0, 1)\n    image = transforms.ToPILImage()(image)\n    image.save(path)\n\n\nMODEL_PATH = \"/kaggle/working/output/model4/oil_painting_model11.pth\"    # Path to trained model\nINPUT_IMAGE = \"/kaggle/input/dog-test-img/Golden-retriever-dog-1362597631o6g.jpg\" # Content Image path\nOUTPUT_IMAGE = \"/kaggle/working/output/model4/stylized_img36.jpg\"            # Output image path\nIMAGE_SIZE = 256 \n\n# Load image and preprocess\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(256),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\n# Load model.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = TransformerNet().to(device)\nmodel.load_state_dict(torch.load(MODEL_PATH, weights_only=True)) #, weights_only=True\nmodel.eval()\n\n# Stylize Output.\ncontent_image = load_image(INPUT_IMAGE).to(device)\nwith torch.no_grad():\n    output = model(content_image)\n\nsave_output(output, OUTPUT_IMAGE)\nprint(f\"Stylized image saved to: {OUTPUT_IMAGE}\")\n# plt.imshow(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:23:08.963746Z","iopub.execute_input":"2025-04-24T07:23:08.964044Z","iopub.status.idle":"2025-04-24T07:23:09.030089Z","shell.execute_reply.started":"2025-04-24T07:23:08.964021Z","shell.execute_reply":"2025-04-24T07:23:09.029370Z"}},"outputs":[{"name":"stdout","text":"Stylized image saved to: /kaggle/working/output/model4/stylized_img35.jpg\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# Display Output Image\n# Load the JPG image\nimg = mpimg.imread('/kaggle/working/output/model4/stylized_img27_dog.jpg')\n\n# Display the image\nplt.imshow(img)\nplt.axis('off') \nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-24T08:11:03.259Z"}},"outputs":[],"execution_count":null}]}