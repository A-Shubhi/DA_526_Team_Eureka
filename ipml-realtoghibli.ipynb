{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11264584,"sourceType":"datasetVersion","datasetId":7040975},{"sourceId":11539173,"sourceType":"datasetVersion","datasetId":7236558},{"sourceId":11540939,"sourceType":"datasetVersion","datasetId":7237588},{"sourceId":11541763,"sourceType":"datasetVersion","datasetId":7238101}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n%cd pytorch-CycleGAN-and-pix2pix\n!pip install -r requirements.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-07T21:02:46.906117Z","iopub.execute_input":"2025-05-07T21:02:46.906342Z","iopub.status.idle":"2025-05-07T21:04:11.253360Z","shell.execute_reply.started":"2025-05-07T21:02:46.906324Z","shell.execute_reply":"2025-05-07T21:04:11.252413Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'pytorch-CycleGAN-and-pix2pix'...\nremote: Enumerating objects: 2516, done.\u001b[K\nremote: Total 2516 (delta 0), reused 0 (delta 0), pack-reused 2516 (from 1)\u001b[K\nReceiving objects: 100% (2516/2516), 8.20 MiB | 34.14 MiB/s, done.\nResolving deltas: 100% (1575/1575), done.\n/kaggle/working/pytorch-CycleGAN-and-pix2pix\nRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.5.1+cu124)\nRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.20.1+cu124)\nCollecting dominate>=2.4.0 (from -r requirements.txt (line 3))\n  Downloading dominate-2.9.1-py2.py3-none-any.whl.metadata (13 kB)\nCollecting visdom>=0.1.8.8 (from -r requirements.txt (line 4))\n  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.19.6)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.4.0->-r requirements.txt (line 1))\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.4.0->-r requirements.txt (line 1))\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.4.0->-r requirements.txt (line 1))\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.4.0->-r requirements.txt (line 1))\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.4.0->-r requirements.txt (line 1))\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.4.0->-r requirements.txt (line 1))\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.4.0->-r requirements.txt (line 1))\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->-r requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (11.1.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.15.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.32.3)\nRequirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (6.4.2)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.17.0)\nRequirement already satisfied: jsonpatch in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.33)\nRequirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.8.0)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (8.1.8)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (3.1.44)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (4.3.7)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (7.0.0)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.11.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (6.0.2)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.21.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (75.1.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.12)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4.0->-r requirements.txt (line 1)) (3.0.2)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.0.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision>=0.5.0->-r requirements.txt (line 2)) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\nBuilding wheels for collected packages: visdom\n  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408196 sha256=f6ebf18a9bee844296b49e614e957521f863c1434f543ec621e3fc106d876784\n  Stored in directory: /root/.cache/pip/wheels/fa/a4/bb/2be445c295d88a74f9c0a4232f04860ca489a5c7c57eb959d9\nSuccessfully built visdom\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, dominate, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, visdom\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed dominate-2.9.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 visdom-0.2.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!mkdir -p ./datasets/ghibli_finetune/trainA\n!mkdir -p ./datasets/ghibli_finetune/trainB\n\n!cp /kaggle/input/real-to-ghibli-image-dataset-5k-paired-images/dataset/trainA/* ./datasets/ghibli_finetune/trainA/\n!cp /kaggle/input/real-to-ghibli-image-dataset-5k-paired-images/dataset/trainB_ghibli/* ./datasets/ghibli_finetune/trainB/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T21:04:16.746910Z","iopub.execute_input":"2025-05-07T21:04:16.747729Z","iopub.status.idle":"2025-05-07T21:04:39.126322Z","shell.execute_reply.started":"2025-05-07T21:04:16.747678Z","shell.execute_reply":"2025-05-07T21:04:39.125421Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!mkdir -p ./checkpoints/ghibli_finetune/\n\n!cp /kaggle/input/modelweights/latest_net_*.pth ./checkpoints/ghibli_finetune/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T21:04:52.525410Z","iopub.execute_input":"2025-05-07T21:04:52.526461Z","iopub.status.idle":"2025-05-07T21:04:54.575836Z","shell.execute_reply.started":"2025-05-07T21:04:52.526421Z","shell.execute_reply":"2025-05-07T21:04:54.575003Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!python train.py \\\n    --dataroot ./datasets/ghibli_finetune \\\n    --name ghibli_finetune \\\n    --model cycle_gan \\\n    --continue_train \\\n    --epoch_count 1 \\\n    --load_size 286 \\\n    --crop_size 256 \\\n    --direction AtoB \\\n    --n_epochs 50 \\\n    --n_epochs_decay 50 \\\n    --save_epoch_freq 5 \\\n    --save_latest_freq 1000","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-07T21:04:56.701685Z","iopub.execute_input":"2025-05-07T21:04:56.701987Z","iopub.status.idle":"2025-05-08T00:56:47.877774Z","shell.execute_reply.started":"2025-05-07T21:04:56.701951Z","shell.execute_reply":"2025-05-08T00:56:47.876712Z"}},"outputs":[{"name":"stdout","text":"----------------- Options ---------------\n               batch_size: 1                             \n                    beta1: 0.5                           \n          checkpoints_dir: ./checkpoints                 \n           continue_train: True                          \t[default: False]\n                crop_size: 256                           \n                 dataroot: ./datasets/ghibli_finetune    \t[default: None]\n             dataset_mode: unaligned                     \n                direction: AtoB                          \n              display_env: main                          \n             display_freq: 400                           \n               display_id: 1                             \n            display_ncols: 4                             \n             display_port: 8097                          \n           display_server: http://localhost              \n          display_winsize: 256                           \n                    epoch: latest                        \n              epoch_count: 1                             \n                 gan_mode: lsgan                         \n                  gpu_ids: 0                             \n                init_gain: 0.02                          \n                init_type: normal                        \n                 input_nc: 3                             \n                  isTrain: True                          \t[default: None]\n                 lambda_A: 10.0                          \n                 lambda_B: 10.0                          \n          lambda_identity: 0.5                           \n                load_iter: 0                             \t[default: 0]\n                load_size: 286                           \n                       lr: 0.0002                        \n           lr_decay_iters: 50                            \n                lr_policy: linear                        \n         max_dataset_size: inf                           \n                    model: cycle_gan                     \n                 n_epochs: 50                            \t[default: 100]\n           n_epochs_decay: 50                            \t[default: 100]\n               n_layers_D: 3                             \n                     name: ghibli_finetune               \t[default: experiment_name]\n                      ndf: 64                            \n                     netD: basic                         \n                     netG: resnet_9blocks                \n                      ngf: 64                            \n               no_dropout: True                          \n                  no_flip: False                         \n                  no_html: False                         \n                     norm: instance                      \n              num_threads: 4                             \n                output_nc: 3                             \n                    phase: train                         \n                pool_size: 50                            \n               preprocess: resize_and_crop               \n               print_freq: 100                           \n             save_by_iter: False                         \n          save_epoch_freq: 5                             \n         save_latest_freq: 1000                          \t[default: 5000]\n           serial_batches: False                         \n                   suffix:                               \n         update_html_freq: 1000                          \n                use_wandb: False                         \n                  verbose: False                         \n       wandb_project_name: CycleGAN-and-pix2pix          \n----------------- End -------------------\ndataset [UnalignedDataset] was created\nThe number of training images = 2500\ninitialize network with normal\ninitialize network with normal\ninitialize network with normal\ninitialize network with normal\nmodel [CycleGANModel] was created\nloading the model from ./checkpoints/ghibli_finetune/latest_net_G_A.pth\n/kaggle/working/pytorch-CycleGAN-and-pix2pix/models/base_model.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(load_path, map_location=str(self.device))\nloading the model from ./checkpoints/ghibli_finetune/latest_net_G_B.pth\nloading the model from ./checkpoints/ghibli_finetune/latest_net_D_A.pth\nloading the model from ./checkpoints/ghibli_finetune/latest_net_D_B.pth\n---------- Networks initialized -------------\n[Network G_A] Total number of parameters : 11.378 M\n[Network G_B] Total number of parameters : 11.378 M\n[Network D_A] Total number of parameters : 2.765 M\n[Network D_B] Total number of parameters : 2.765 M\n-----------------------------------------------\nException in user code:\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 198, in _new_conn\n    sock = connection.create_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\", line 85, in create_connection\n    raise err\n  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/connection.py\", line 73, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 787, in urlopen\n    response = self._make_request(\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 493, in _make_request\n    conn.request(\n  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 445, in request\n    self.endheaders()\n  File \"/usr/lib/python3.11/http/client.py\", line 1298, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/usr/lib/python3.11/http/client.py\", line 1058, in _send_output\n    self.send(msg)\n  File \"/usr/lib/python3.11/http/client.py\", line 996, in send\n    self.connect()\n  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 276, in connect\n    self.sock = self._new_conn()\n                ^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\", line 213, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f800072d950>: Failed to establish a new connection: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n           ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\", line 841, in urlopen\n    retries = retries.increment(\n              ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/urllib3/util/retry.py\", line 519, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f800072d950>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/visdom/__init__.py\", line 756, in _send\n    return self._handle_post(\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/visdom/__init__.py\", line 720, in _handle_post\n    r = self.session.post(url, data=data)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 700, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='localhost', port=8097): Max retries exceeded with url: /env/main (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f800072d950>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\n\nCould not connect to Visdom server. \n Trying to start a server....\nCommand: /usr/bin/python3 -m visdom.server -p 8097 &>/dev/null &\ncreate web directory ./checkpoints/ghibli_finetune/web...\n/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  warnings.warn(\nlearning rate 0.0002000 -> 0.0002000\n(epoch: 1, iters: 100, time: 0.631, data: 0.327) D_A: 0.296 G_A: 0.181 cycle_A: 0.994 idt_A: 0.377 D_B: 0.155 G_B: 0.360 cycle_B: 0.819 idt_B: 0.440 \n(epoch: 1, iters: 200, time: 0.690, data: 0.002) D_A: 0.208 G_A: 0.165 cycle_A: 1.316 idt_A: 0.934 D_B: 0.194 G_B: 0.287 cycle_B: 1.280 idt_B: 0.671 \n(epoch: 1, iters: 300, time: 0.678, data: 0.003) D_A: 0.253 G_A: 0.904 cycle_A: 3.556 idt_A: 0.619 D_B: 0.245 G_B: 0.137 cycle_B: 1.000 idt_B: 1.017 \n(epoch: 1, iters: 400, time: 1.205, data: 0.002) D_A: 0.193 G_A: 0.258 cycle_A: 0.603 idt_A: 0.854 D_B: 0.247 G_B: 0.335 cycle_B: 1.823 idt_B: 0.285 \n(epoch: 1, iters: 500, time: 0.679, data: 0.002) D_A: 0.205 G_A: 0.237 cycle_A: 0.987 idt_A: 0.724 D_B: 0.084 G_B: 0.166 cycle_B: 2.010 idt_B: 0.394 \n(epoch: 1, iters: 600, time: 0.683, data: 0.002) D_A: 0.062 G_A: 0.810 cycle_A: 1.348 idt_A: 0.842 D_B: 0.068 G_B: 0.762 cycle_B: 1.891 idt_B: 0.730 \n(epoch: 1, iters: 700, time: 0.684, data: 0.002) D_A: 0.092 G_A: 0.608 cycle_A: 0.744 idt_A: 1.017 D_B: 0.088 G_B: 0.500 cycle_B: 1.936 idt_B: 0.329 \n(epoch: 1, iters: 800, time: 0.968, data: 0.002) D_A: 0.150 G_A: 0.442 cycle_A: 0.872 idt_A: 0.425 D_B: 0.073 G_B: 0.475 cycle_B: 0.807 idt_B: 0.315 \n(epoch: 1, iters: 900, time: 0.678, data: 0.002) D_A: 0.146 G_A: 0.679 cycle_A: 2.223 idt_A: 0.388 D_B: 0.136 G_B: 0.339 cycle_B: 0.748 idt_B: 1.192 \n(epoch: 1, iters: 1000, time: 0.685, data: 0.002) D_A: 0.058 G_A: 0.527 cycle_A: 0.717 idt_A: 0.347 D_B: 0.181 G_B: 0.540 cycle_B: 0.872 idt_B: 0.279 \nsaving the latest model (epoch 1, total_iters 1000)\n(epoch: 1, iters: 1100, time: 0.684, data: 0.002) D_A: 0.041 G_A: 0.645 cycle_A: 0.693 idt_A: 0.655 D_B: 0.312 G_B: 1.320 cycle_B: 1.070 idt_B: 0.298 \n(epoch: 1, iters: 1200, time: 0.907, data: 0.002) D_A: 0.075 G_A: 0.243 cycle_A: 1.163 idt_A: 0.579 D_B: 0.086 G_B: 0.633 cycle_B: 0.997 idt_B: 0.654 \n(epoch: 1, iters: 1300, time: 0.684, data: 0.002) D_A: 0.239 G_A: 0.538 cycle_A: 2.033 idt_A: 0.733 D_B: 0.082 G_B: 0.035 cycle_B: 1.272 idt_B: 0.615 \n(epoch: 1, iters: 1400, time: 0.686, data: 0.002) D_A: 0.111 G_A: 0.119 cycle_A: 0.964 idt_A: 0.686 D_B: 0.167 G_B: 0.307 cycle_B: 1.219 idt_B: 0.546 \n(epoch: 1, iters: 1500, time: 0.686, data: 0.003) D_A: 0.334 G_A: 0.397 cycle_A: 1.054 idt_A: 0.609 D_B: 0.154 G_B: 0.296 cycle_B: 1.045 idt_B: 0.463 \n(epoch: 1, iters: 1600, time: 0.918, data: 0.002) D_A: 0.277 G_A: 0.153 cycle_A: 1.273 idt_A: 0.586 D_B: 0.365 G_B: 0.118 cycle_B: 1.209 idt_B: 0.428 \n(epoch: 1, iters: 1700, time: 0.683, data: 0.002) D_A: 0.070 G_A: 0.125 cycle_A: 2.165 idt_A: 0.270 D_B: 0.148 G_B: 0.692 cycle_B: 0.532 idt_B: 1.069 \n(epoch: 1, iters: 1800, time: 0.686, data: 0.002) D_A: 0.363 G_A: 0.452 cycle_A: 1.777 idt_A: 0.822 D_B: 0.378 G_B: 0.087 cycle_B: 1.269 idt_B: 0.423 \n(epoch: 1, iters: 1900, time: 0.683, data: 0.002) D_A: 0.227 G_A: 0.248 cycle_A: 0.843 idt_A: 0.727 D_B: 0.127 G_B: 0.192 cycle_B: 1.383 idt_B: 0.281 \n(epoch: 1, iters: 2100, time: 0.685, data: 0.002) D_A: 0.343 G_A: 0.534 cycle_A: 0.866 idt_A: 0.505 D_B: 0.240 G_B: 0.243 cycle_B: 0.855 idt_B: 0.326 \n(epoch: 1, iters: 2200, time: 0.682, data: 0.002) D_A: 0.072 G_A: 0.747 cycle_A: 1.140 idt_A: 0.421 D_B: 0.094 G_B: 0.655 cycle_B: 0.824 idt_B: 0.405 \n(epoch: 1, iters: 2300, time: 0.686, data: 0.002) D_A: 0.099 G_A: 0.732 cycle_A: 0.801 idt_A: 0.590 D_B: 0.185 G_B: 0.639 cycle_B: 1.062 idt_B: 0.256 \n(epoch: 1, iters: 2400, time: 0.929, data: 0.002) D_A: 0.301 G_A: 0.657 cycle_A: 0.899 idt_A: 0.542 D_B: 0.162 G_B: 0.392 cycle_B: 0.934 idt_B: 0.632 \n(epoch: 1, iters: 2500, time: 0.683, data: 0.002) D_A: 0.368 G_A: 0.657 cycle_A: 1.208 idt_A: 0.726 D_B: 0.036 G_B: 0.267 cycle_B: 1.663 idt_B: 0.679 \nEnd of epoch 1 / 100 \t Time Taken: 1369 sec\nlearning rate 0.0002000 -> 0.0002000\n(epoch: 2, iters: 100, time: 0.682, data: 0.145) D_A: 0.351 G_A: 0.320 cycle_A: 0.837 idt_A: 0.763 D_B: 0.243 G_B: 0.379 cycle_B: 1.681 idt_B: 0.216 \n(epoch: 2, iters: 200, time: 0.681, data: 0.003) D_A: 0.189 G_A: 0.221 cycle_A: 1.209 idt_A: 3.163 D_B: 0.175 G_B: 0.243 cycle_B: 4.462 idt_B: 0.454 \n(epoch: 2, iters: 300, time: 1.160, data: 0.002) D_A: 0.154 G_A: 0.308 cycle_A: 0.771 idt_A: 0.586 D_B: 0.123 G_B: 0.621 cycle_B: 1.129 idt_B: 0.282 \n(epoch: 2, iters: 400, time: 0.682, data: 0.002) D_A: 0.333 G_A: 0.194 cycle_A: 0.798 idt_A: 0.362 D_B: 0.374 G_B: 0.603 cycle_B: 0.763 idt_B: 0.470 \n(epoch: 2, iters: 500, time: 0.682, data: 0.002) D_A: 0.072 G_A: 0.547 cycle_A: 0.621 idt_A: 0.324 D_B: 0.096 G_B: 0.592 cycle_B: 0.855 idt_B: 0.293 \nsaving the latest model (epoch 2, total_iters 3000)\n(epoch: 2, iters: 600, time: 0.683, data: 0.002) D_A: 0.166 G_A: 0.530 cycle_A: 1.021 idt_A: 0.340 D_B: 0.096 G_B: 0.182 cycle_B: 0.750 idt_B: 0.376 \n(epoch: 2, iters: 700, time: 0.920, data: 0.002) D_A: 0.154 G_A: 0.342 cycle_A: 1.179 idt_A: 0.467 D_B: 0.196 G_B: 0.298 cycle_B: 0.837 idt_B: 0.546 \n(epoch: 2, iters: 800, time: 0.683, data: 0.002) D_A: 0.143 G_A: 0.415 cycle_A: 0.762 idt_A: 0.372 D_B: 0.299 G_B: 0.403 cycle_B: 0.847 idt_B: 0.333 \n(epoch: 2, iters: 900, time: 0.682, data: 0.002) D_A: 0.094 G_A: 0.273 cycle_A: 0.695 idt_A: 0.348 D_B: 0.117 G_B: 0.635 cycle_B: 0.879 idt_B: 0.318 \n(epoch: 2, iters: 1000, time: 0.683, data: 0.002) D_A: 0.144 G_A: 0.202 cycle_A: 0.838 idt_A: 0.728 D_B: 0.326 G_B: 0.268 cycle_B: 1.553 idt_B: 0.392 \n(epoch: 2, iters: 1100, time: 0.945, data: 0.002) D_A: 0.108 G_A: 0.424 cycle_A: 0.886 idt_A: 0.870 D_B: 0.213 G_B: 0.633 cycle_B: 2.091 idt_B: 0.385 \n(epoch: 2, iters: 1200, time: 0.682, data: 0.002) D_A: 0.177 G_A: 0.599 cycle_A: 3.140 idt_A: 0.547 D_B: 0.135 G_B: 0.363 cycle_B: 0.904 idt_B: 1.279 \n(epoch: 2, iters: 1500, time: 1.137, data: 0.002) D_A: 0.130 G_A: 0.277 cycle_A: 0.704 idt_A: 0.415 D_B: 0.174 G_B: 0.349 cycle_B: 1.580 idt_B: 0.197 \nsaving the latest model (epoch 2, total_iters 4000)\n(epoch: 2, iters: 1600, time: 0.682, data: 0.002) D_A: 0.110 G_A: 0.492 cycle_A: 0.698 idt_A: 0.316 D_B: 0.216 G_B: 0.322 cycle_B: 0.736 idt_B: 0.301 \n(epoch: 2, iters: 1700, time: 0.683, data: 0.002) D_A: 0.078 G_A: 0.449 cycle_A: 1.365 idt_A: 1.249 D_B: 0.066 G_B: 0.549 cycle_B: 2.607 idt_B: 0.391 \n(epoch: 2, iters: 1800, time: 0.684, data: 0.002) D_A: 0.121 G_A: 0.189 cycle_A: 1.705 idt_A: 1.510 D_B: 0.309 G_B: 0.300 cycle_B: 2.558 idt_B: 0.646 \n(epoch: 2, iters: 1900, time: 0.955, data: 0.003) D_A: 0.282 G_A: 0.558 cycle_A: 0.937 idt_A: 0.311 D_B: 0.206 G_B: 0.265 cycle_B: 1.005 idt_B: 0.395 \n(epoch: 2, iters: 2000, time: 0.682, data: 0.003) D_A: 0.304 G_A: 0.576 cycle_A: 0.747 idt_A: 0.472 D_B: 0.153 G_B: 0.420 cycle_B: 0.882 idt_B: 0.410 \n(epoch: 2, iters: 2100, time: 0.685, data: 0.002) D_A: 0.079 G_A: 0.408 cycle_A: 0.925 idt_A: 0.438 D_B: 0.087 G_B: 0.740 cycle_B: 1.146 idt_B: 0.343 \n(epoch: 2, iters: 2200, time: 0.687, data: 0.002) D_A: 0.235 G_A: 0.160 cycle_A: 0.959 idt_A: 0.351 D_B: 0.291 G_B: 0.444 cycle_B: 1.090 idt_B: 0.479 \n(epoch: 2, iters: 2300, time: 0.944, data: 0.002) D_A: 0.178 G_A: 0.441 cycle_A: 2.445 idt_A: 0.567 D_B: 0.218 G_B: 0.283 cycle_B: 1.343 idt_B: 1.048 \n(epoch: 2, iters: 2400, time: 0.683, data: 0.002) D_A: 0.087 G_A: 0.561 cycle_A: 0.993 idt_A: 0.358 D_B: 0.095 G_B: 0.395 cycle_B: 0.876 idt_B: 0.444 \n(epoch: 2, iters: 2500, time: 0.684, data: 0.002) D_A: 0.150 G_A: 0.219 cycle_A: 1.123 idt_A: 0.273 D_B: 0.153 G_B: 0.733 cycle_B: 0.566 idt_B: 0.640 \nsaving the latest model (epoch 2, total_iters 5000)\nEnd of epoch 2 / 100 \t Time Taken: 1375 sec\nlearning rate 0.0002000 -> 0.0002000\n(epoch: 3, iters: 100, time: 0.686, data: 0.158) D_A: 0.412 G_A: 0.138 cycle_A: 0.661 idt_A: 0.501 D_B: 0.346 G_B: 0.126 cycle_B: 0.897 idt_B: 0.292 \n(epoch: 3, iters: 200, time: 1.239, data: 0.002) D_A: 0.174 G_A: 0.264 cycle_A: 0.877 idt_A: 0.366 D_B: 0.182 G_B: 0.474 cycle_B: 0.889 idt_B: 0.354 \n(epoch: 3, iters: 300, time: 0.686, data: 0.002) D_A: 0.200 G_A: 0.409 cycle_A: 1.213 idt_A: 0.746 D_B: 0.203 G_B: 0.427 cycle_B: 1.942 idt_B: 0.492 \n(epoch: 3, iters: 400, time: 0.683, data: 0.002) D_A: 0.034 G_A: 0.427 cycle_A: 0.731 idt_A: 0.839 D_B: 0.311 G_B: 0.606 cycle_B: 2.003 idt_B: 0.427 \n(epoch: 3, iters: 500, time: 0.685, data: 0.002) D_A: 0.244 G_A: 0.581 cycle_A: 0.886 idt_A: 0.689 D_B: 0.137 G_B: 0.337 cycle_B: 1.477 idt_B: 0.526 \n(epoch: 3, iters: 600, time: 0.939, data: 0.002) D_A: 0.195 G_A: 0.364 cycle_A: 1.122 idt_A: 0.639 D_B: 0.060 G_B: 1.029 cycle_B: 1.366 idt_B: 0.394 \n(epoch: 3, iters: 1000, time: 1.257, data: 0.002) D_A: 0.171 G_A: 0.286 cycle_A: 1.603 idt_A: 0.284 D_B: 0.373 G_B: 0.152 cycle_B: 0.583 idt_B: 0.856 \nsaving the latest model (epoch 3, total_iters 6000)\n(epoch: 3, iters: 1100, time: 0.685, data: 0.002) D_A: 0.315 G_A: 0.088 cycle_A: 0.515 idt_A: 0.545 D_B: 0.166 G_B: 0.366 cycle_B: 0.970 idt_B: 0.379 \n(epoch: 3, iters: 1200, time: 0.689, data: 0.002) D_A: 0.177 G_A: 0.278 cycle_A: 1.045 idt_A: 0.553 D_B: 0.180 G_B: 0.451 cycle_B: 1.083 idt_B: 0.540 \n(epoch: 3, iters: 1300, time: 0.683, data: 0.002) D_A: 0.306 G_A: 0.619 cycle_A: 0.693 idt_A: 0.434 D_B: 0.216 G_B: 0.190 cycle_B: 0.766 idt_B: 0.422 \n(epoch: 3, iters: 1400, time: 0.961, data: 0.002) D_A: 0.193 G_A: 0.309 cycle_A: 0.574 idt_A: 1.015 D_B: 0.139 G_B: 0.269 cycle_B: 2.049 idt_B: 0.335 \n(epoch: 3, iters: 1500, time: 0.682, data: 0.002) D_A: 0.102 G_A: 0.164 cycle_A: 0.727 idt_A: 0.428 D_B: 0.348 G_B: 0.546 cycle_B: 1.206 idt_B: 0.343 \n(epoch: 3, iters: 1600, time: 0.688, data: 0.002) D_A: 0.214 G_A: 0.203 cycle_A: 1.648 idt_A: 0.412 D_B: 0.288 G_B: 0.555 cycle_B: 1.023 idt_B: 0.836 \n(epoch: 3, iters: 1700, time: 0.685, data: 0.003) D_A: 0.265 G_A: 0.166 cycle_A: 1.673 idt_A: 0.923 D_B: 0.305 G_B: 0.029 cycle_B: 1.703 idt_B: 0.857 \n(epoch: 3, iters: 1800, time: 0.949, data: 0.002) D_A: 0.115 G_A: 0.941 cycle_A: 1.785 idt_A: 0.449 D_B: 0.117 G_B: 0.358 cycle_B: 0.771 idt_B: 0.539 \n(epoch: 3, iters: 1900, time: 0.683, data: 0.003) D_A: 0.175 G_A: 0.074 cycle_A: 0.830 idt_A: 1.105 D_B: 0.316 G_B: 0.295 cycle_B: 1.890 idt_B: 0.380 \n(epoch: 3, iters: 2000, time: 0.681, data: 0.002) D_A: 0.259 G_A: 0.747 cycle_A: 0.553 idt_A: 0.587 D_B: 0.113 G_B: 0.549 cycle_B: 0.898 idt_B: 0.235 \nsaving the latest model (epoch 3, total_iters 7000)\n(epoch: 3, iters: 2100, time: 0.684, data: 0.002) D_A: 0.235 G_A: 0.373 cycle_A: 0.868 idt_A: 0.745 D_B: 0.300 G_B: 0.295 cycle_B: 1.690 idt_B: 0.451 \n(epoch: 3, iters: 2200, time: 0.918, data: 0.002) D_A: 0.171 G_A: 0.439 cycle_A: 1.166 idt_A: 0.252 D_B: 0.059 G_B: 0.558 cycle_B: 0.708 idt_B: 0.575 \n(epoch: 3, iters: 2300, time: 0.689, data: 0.002) D_A: 0.126 G_A: 0.172 cycle_A: 0.826 idt_A: 0.346 D_B: 0.152 G_B: 0.728 cycle_B: 0.772 idt_B: 0.418 \n(epoch: 3, iters: 2500, time: 0.681, data: 0.002) D_A: 0.206 G_A: 0.309 cycle_A: 0.665 idt_A: 1.358 D_B: 0.184 G_B: 0.507 cycle_B: 2.355 idt_B: 0.412 \nEnd of epoch 3 / 100 \t Time Taken: 1374 sec\nlearning rate 0.0002000 -> 0.0002000\n(epoch: 4, iters: 100, time: 1.218, data: 0.125) D_A: 0.058 G_A: 0.547 cycle_A: 1.133 idt_A: 0.436 D_B: 0.193 G_B: 0.417 cycle_B: 0.831 idt_B: 0.555 \n(epoch: 4, iters: 200, time: 0.684, data: 0.002) D_A: 0.251 G_A: 0.414 cycle_A: 0.803 idt_A: 0.339 D_B: 0.233 G_B: 0.176 cycle_B: 0.838 idt_B: 0.318 \n(epoch: 4, iters: 300, time: 0.685, data: 0.002) D_A: 0.274 G_A: 0.645 cycle_A: 0.621 idt_A: 1.051 D_B: 0.027 G_B: 0.214 cycle_B: 1.961 idt_B: 0.243 \n(epoch: 4, iters: 400, time: 0.686, data: 0.002) D_A: 0.228 G_A: 0.377 cycle_A: 1.946 idt_A: 0.384 D_B: 0.115 G_B: 0.412 cycle_B: 0.749 idt_B: 1.068 \n(epoch: 4, iters: 500, time: 1.164, data: 0.002) D_A: 0.182 G_A: 0.520 cycle_A: 0.826 idt_A: 0.474 D_B: 0.190 G_B: 0.260 cycle_B: 0.882 idt_B: 0.378 \nsaving the latest model (epoch 4, total_iters 8000)\n(epoch: 4, iters: 600, time: 0.682, data: 0.002) D_A: 0.188 G_A: 0.503 cycle_A: 0.969 idt_A: 0.432 D_B: 0.112 G_B: 0.869 cycle_B: 0.859 idt_B: 0.377 \n(epoch: 4, iters: 700, time: 0.685, data: 0.002) D_A: 0.074 G_A: 0.701 cycle_A: 0.909 idt_A: 0.439 D_B: 0.044 G_B: 0.256 cycle_B: 1.099 idt_B: 0.310 \n(epoch: 4, iters: 800, time: 0.686, data: 0.002) D_A: 0.125 G_A: 1.090 cycle_A: 0.885 idt_A: 0.507 D_B: 0.076 G_B: 0.524 cycle_B: 1.217 idt_B: 0.265 \n(epoch: 4, iters: 900, time: 0.921, data: 0.002) D_A: 0.200 G_A: 0.333 cycle_A: 1.076 idt_A: 0.365 D_B: 0.384 G_B: 0.306 cycle_B: 0.878 idt_B: 0.401 \n(epoch: 4, iters: 1000, time: 0.685, data: 0.002) D_A: 0.286 G_A: 0.634 cycle_A: 0.916 idt_A: 0.492 D_B: 0.097 G_B: 0.288 cycle_B: 1.043 idt_B: 0.247 \n(epoch: 4, iters: 1100, time: 0.688, data: 0.002) D_A: 0.155 G_A: 0.483 cycle_A: 0.857 idt_A: 0.541 D_B: 0.166 G_B: 0.313 cycle_B: 1.315 idt_B: 0.267 \n(epoch: 4, iters: 1200, time: 0.684, data: 0.002) D_A: 0.242 G_A: 0.993 cycle_A: 1.530 idt_A: 0.835 D_B: 0.273 G_B: 0.102 cycle_B: 1.775 idt_B: 0.540 \n(epoch: 4, iters: 1300, time: 0.945, data: 0.002) D_A: 0.495 G_A: 0.275 cycle_A: 1.042 idt_A: 0.498 D_B: 0.308 G_B: 0.080 cycle_B: 1.117 idt_B: 0.589 \n(epoch: 4, iters: 1400, time: 0.687, data: 0.002) D_A: 0.217 G_A: 0.300 cycle_A: 1.250 idt_A: 0.646 D_B: 0.065 G_B: 0.615 cycle_B: 1.337 idt_B: 0.491 \n(epoch: 4, iters: 1500, time: 0.684, data: 0.002) D_A: 0.046 G_A: 0.046 cycle_A: 0.976 idt_A: 0.490 D_B: 0.260 G_B: 0.935 cycle_B: 1.294 idt_B: 0.380 \nsaving the latest model (epoch 4, total_iters 9000)\n(epoch: 4, iters: 1600, time: 0.685, data: 0.002) D_A: 0.212 G_A: 0.157 cycle_A: 1.089 idt_A: 0.709 D_B: 0.170 G_B: 0.359 cycle_B: 1.460 idt_B: 0.415 \n(epoch: 4, iters: 1700, time: 0.920, data: 0.002) D_A: 0.228 G_A: 0.150 cycle_A: 0.822 idt_A: 0.722 D_B: 0.222 G_B: 0.323 cycle_B: 1.424 idt_B: 0.343 \n(epoch: 4, iters: 1800, time: 0.683, data: 0.002) D_A: 0.164 G_A: 0.301 cycle_A: 0.936 idt_A: 0.504 D_B: 0.124 G_B: 0.378 cycle_B: 1.112 idt_B: 0.319 \n(epoch: 4, iters: 1900, time: 0.683, data: 0.002) D_A: 0.172 G_A: 0.134 cycle_A: 0.675 idt_A: 0.419 D_B: 0.342 G_B: 0.231 cycle_B: 1.012 idt_B: 0.382 \n(epoch: 4, iters: 2000, time: 0.682, data: 0.002) D_A: 0.044 G_A: 0.490 cycle_A: 0.726 idt_A: 0.411 D_B: 0.087 G_B: 0.755 cycle_B: 0.806 idt_B: 0.302 \n(epoch: 4, iters: 2100, time: 0.946, data: 0.002) D_A: 0.303 G_A: 0.476 cycle_A: 1.158 idt_A: 0.594 D_B: 0.312 G_B: 0.184 cycle_B: 0.652 idt_B: 0.433 \n(epoch: 4, iters: 2200, time: 0.683, data: 0.002) D_A: 0.436 G_A: 0.361 cycle_A: 0.686 idt_A: 0.856 D_B: 0.178 G_B: 0.616 cycle_B: 1.804 idt_B: 0.315 \n(epoch: 4, iters: 2300, time: 0.686, data: 0.002) D_A: 0.255 G_A: 0.114 cycle_A: 1.149 idt_A: 0.270 D_B: 0.239 G_B: 0.405 cycle_B: 0.692 idt_B: 0.447 \n(epoch: 5, iters: 100, time: 0.685, data: 0.151) D_A: 0.097 G_A: 1.176 cycle_A: 1.390 idt_A: 0.524 D_B: 0.071 G_B: 0.738 cycle_B: 1.058 idt_B: 0.462 \n(epoch: 5, iters: 200, time: 0.684, data: 0.002) D_A: 0.234 G_A: 0.189 cycle_A: 0.752 idt_A: 0.443 D_B: 0.265 G_B: 0.190 cycle_B: 1.240 idt_B: 0.335 \n(epoch: 5, iters: 300, time: 0.687, data: 0.002) D_A: 0.049 G_A: 0.672 cycle_A: 1.110 idt_A: 0.399 D_B: 0.228 G_B: 0.328 cycle_B: 1.381 idt_B: 0.547 \n(epoch: 5, iters: 400, time: 1.174, data: 0.002) D_A: 0.403 G_A: 0.237 cycle_A: 1.198 idt_A: 0.466 D_B: 0.330 G_B: 0.530 cycle_B: 1.071 idt_B: 0.507 \n(epoch: 5, iters: 500, time: 0.687, data: 0.002) D_A: 0.018 G_A: 0.165 cycle_A: 1.302 idt_A: 0.479 D_B: 0.132 G_B: 0.325 cycle_B: 1.235 idt_B: 0.289 \n(epoch: 5, iters: 600, time: 0.683, data: 0.002) D_A: 0.159 G_A: 0.132 cycle_A: 0.799 idt_A: 0.714 D_B: 0.052 G_B: 0.157 cycle_B: 1.461 idt_B: 0.234 \n(epoch: 5, iters: 700, time: 0.685, data: 0.002) D_A: 0.132 G_A: 0.187 cycle_A: 0.483 idt_A: 0.525 D_B: 0.131 G_B: 0.748 cycle_B: 0.850 idt_B: 0.394 \n(epoch: 5, iters: 800, time: 0.891, data: 0.002) D_A: 0.280 G_A: 0.133 cycle_A: 1.323 idt_A: 0.383 D_B: 0.343 G_B: 0.566 cycle_B: 0.844 idt_B: 0.541 \n(epoch: 5, iters: 900, time: 0.686, data: 0.002) D_A: 0.254 G_A: 0.351 cycle_A: 0.886 idt_A: 0.496 D_B: 0.445 G_B: 0.123 cycle_B: 1.388 idt_B: 0.520 \n(epoch: 5, iters: 1000, time: 0.682, data: 0.002) D_A: 0.183 G_A: 0.427 cycle_A: 0.690 idt_A: 1.430 D_B: 0.083 G_B: 0.235 cycle_B: 2.772 idt_B: 0.254 \nsaving the latest model (epoch 5, total_iters 11000)\n(epoch: 5, iters: 1100, time: 0.684, data: 0.002) D_A: 0.315 G_A: 0.826 cycle_A: 1.454 idt_A: 0.408 D_B: 0.233 G_B: 0.204 cycle_B: 0.858 idt_B: 0.731 \n(epoch: 5, iters: 1200, time: 0.957, data: 0.002) D_A: 0.253 G_A: 0.126 cycle_A: 0.851 idt_A: 0.483 D_B: 0.125 G_B: 0.203 cycle_B: 1.094 idt_B: 0.499 \n(epoch: 5, iters: 1300, time: 0.684, data: 0.002) D_A: 0.184 G_A: 1.008 cycle_A: 1.017 idt_A: 0.591 D_B: 0.144 G_B: 0.402 cycle_B: 1.678 idt_B: 0.351 \n(epoch: 5, iters: 1400, time: 0.682, data: 0.002) D_A: 0.247 G_A: 0.171 cycle_A: 1.392 idt_A: 0.273 D_B: 0.048 G_B: 0.534 cycle_B: 0.673 idt_B: 0.787 \n(epoch: 5, iters: 1500, time: 0.682, data: 0.002) D_A: 0.122 G_A: 0.543 cycle_A: 0.963 idt_A: 0.514 D_B: 0.060 G_B: 0.287 cycle_B: 1.284 idt_B: 0.414 \n(epoch: 5, iters: 1600, time: 0.925, data: 0.002) D_A: 0.376 G_A: 0.356 cycle_A: 0.925 idt_A: 0.516 D_B: 0.102 G_B: 0.419 cycle_B: 0.997 idt_B: 0.426 \n(epoch: 5, iters: 1700, time: 0.683, data: 0.002) D_A: 0.153 G_A: 0.142 cycle_A: 0.746 idt_A: 0.747 D_B: 0.140 G_B: 0.708 cycle_B: 1.078 idt_B: 0.299 \n(epoch: 5, iters: 1800, time: 0.685, data: 0.003) D_A: 0.168 G_A: 0.874 cycle_A: 0.933 idt_A: 0.394 D_B: 0.122 G_B: 0.577 cycle_B: 0.979 idt_B: 0.396 \n(epoch: 5, iters: 1900, time: 0.684, data: 0.002) D_A: 0.490 G_A: 0.142 cycle_A: 1.507 idt_A: 0.660 D_B: 0.289 G_B: 0.314 cycle_B: 1.743 idt_B: 0.657 \n(epoch: 5, iters: 2000, time: 1.143, data: 0.002) D_A: 0.148 G_A: 0.554 cycle_A: 0.816 idt_A: 0.323 D_B: 0.106 G_B: 0.592 cycle_B: 1.109 idt_B: 0.401 \nsaving the latest model (epoch 5, total_iters 12000)\n(epoch: 5, iters: 2100, time: 0.685, data: 0.002) D_A: 0.205 G_A: 0.481 cycle_A: 1.212 idt_A: 0.317 D_B: 0.234 G_B: 0.209 cycle_B: 0.722 idt_B: 0.655 \n(epoch: 5, iters: 2200, time: 0.681, data: 0.002) D_A: 0.220 G_A: 0.410 cycle_A: 0.957 idt_A: 0.416 D_B: 0.093 G_B: 0.602 cycle_B: 1.199 idt_B: 0.431 \n(epoch: 5, iters: 2300, time: 0.686, data: 0.002) D_A: 0.136 G_A: 0.300 cycle_A: 2.529 idt_A: 0.655 D_B: 0.038 G_B: 0.563 cycle_B: 1.476 idt_B: 1.131 \n(epoch: 5, iters: 2400, time: 0.928, data: 0.002) D_A: 0.072 G_A: 0.262 cycle_A: 0.909 idt_A: 0.311 D_B: 0.377 G_B: 0.672 cycle_B: 0.712 idt_B: 0.597 \n(epoch: 5, iters: 2500, time: 0.683, data: 0.002) D_A: 0.205 G_A: 0.219 cycle_A: 1.435 idt_A: 1.183 D_B: 0.166 G_B: 0.606 cycle_B: 2.835 idt_B: 0.488 \nsaving the model at the end of epoch 5, iters 12500\nEnd of epoch 5 / 100 \t Time Taken: 1376 sec\nlearning rate 0.0002000 -> 0.0002000\n(epoch: 6, iters: 100, time: 0.685, data: 0.154) D_A: 0.157 G_A: 0.265 cycle_A: 1.559 idt_A: 0.558 D_B: 0.312 G_B: 0.869 cycle_B: 1.239 idt_B: 0.748 \n(epoch: 6, iters: 200, time: 0.682, data: 0.002) D_A: 0.022 G_A: 0.539 cycle_A: 1.066 idt_A: 0.618 D_B: 0.113 G_B: 0.700 cycle_B: 1.482 idt_B: 0.458 \n(epoch: 6, iters: 300, time: 1.118, data: 0.002) D_A: 0.110 G_A: 0.218 cycle_A: 1.513 idt_A: 0.442 D_B: 0.114 G_B: 0.747 cycle_B: 1.001 idt_B: 0.837 \n(epoch: 6, iters: 400, time: 0.687, data: 0.002) D_A: 0.304 G_A: 0.137 cycle_A: 0.812 idt_A: 0.379 D_B: 0.328 G_B: 0.682 cycle_B: 0.764 idt_B: 0.379 \n(epoch: 6, iters: 500, time: 0.684, data: 0.002) D_A: 0.161 G_A: 0.756 cycle_A: 0.841 idt_A: 0.388 D_B: 0.085 G_B: 0.708 cycle_B: 0.765 idt_B: 0.330 \nsaving the latest model (epoch 6, total_iters 13000)\n(epoch: 6, iters: 600, time: 0.684, data: 0.002) D_A: 0.111 G_A: 0.771 cycle_A: 1.072 idt_A: 0.395 D_B: 0.208 G_B: 0.434 cycle_B: 1.040 idt_B: 0.594 \n(epoch: 6, iters: 700, time: 0.936, data: 0.002) D_A: 0.140 G_A: 0.630 cycle_A: 0.715 idt_A: 0.679 D_B: 0.480 G_B: 0.541 cycle_B: 1.065 idt_B: 0.477 \n(epoch: 6, iters: 800, time: 0.686, data: 0.002) D_A: 0.313 G_A: 0.353 cycle_A: 1.223 idt_A: 0.911 D_B: 0.451 G_B: 0.162 cycle_B: 1.801 idt_B: 0.470 \n(epoch: 6, iters: 900, time: 0.688, data: 0.002) D_A: 0.048 G_A: 0.225 cycle_A: 0.830 idt_A: 0.435 D_B: 0.350 G_B: 0.450 cycle_B: 1.242 idt_B: 0.374 \n(epoch: 6, iters: 1000, time: 0.685, data: 0.002) D_A: 0.234 G_A: 0.756 cycle_A: 0.527 idt_A: 0.607 D_B: 0.077 G_B: 0.243 cycle_B: 1.294 idt_B: 0.232 \n(epoch: 6, iters: 1100, time: 0.913, data: 0.002) D_A: 0.239 G_A: 0.193 cycle_A: 1.442 idt_A: 0.441 D_B: 0.180 G_B: 0.585 cycle_B: 0.697 idt_B: 0.529 \n(epoch: 6, iters: 1200, time: 0.685, data: 0.002) D_A: 0.105 G_A: 0.674 cycle_A: 0.789 idt_A: 0.504 D_B: 0.100 G_B: 0.344 cycle_B: 0.940 idt_B: 0.339 \n(epoch: 6, iters: 1300, time: 0.689, data: 0.004) D_A: 0.119 G_A: 0.457 cycle_A: 0.633 idt_A: 0.502 D_B: 0.045 G_B: 0.713 cycle_B: 1.026 idt_B: 0.382 \n(epoch: 6, iters: 1400, time: 0.682, data: 0.002) D_A: 0.297 G_A: 0.258 cycle_A: 0.966 idt_A: 0.500 D_B: 0.185 G_B: 0.156 cycle_B: 0.785 idt_B: 0.445 \n(epoch: 6, iters: 1500, time: 1.185, data: 0.002) D_A: 0.201 G_A: 0.605 cycle_A: 1.670 idt_A: 0.484 D_B: 0.239 G_B: 0.405 cycle_B: 1.213 idt_B: 0.897 \nsaving the latest model (epoch 6, total_iters 14000)\n(epoch: 6, iters: 1600, time: 0.687, data: 0.002) D_A: 0.302 G_A: 0.144 cycle_A: 0.911 idt_A: 0.328 D_B: 0.303 G_B: 0.632 cycle_B: 0.897 idt_B: 0.418 \n(epoch: 6, iters: 1700, time: 0.687, data: 0.002) D_A: 0.273 G_A: 0.457 cycle_A: 1.600 idt_A: 1.256 D_B: 0.227 G_B: 0.432 cycle_B: 2.414 idt_B: 0.833 \n(epoch: 6, iters: 1800, time: 0.685, data: 0.002) D_A: 0.378 G_A: 0.481 cycle_A: 0.914 idt_A: 0.339 D_B: 0.253 G_B: 0.207 cycle_B: 0.706 idt_B: 0.324 \n(epoch: 6, iters: 1900, time: 0.917, data: 0.002) D_A: 0.155 G_A: 0.301 cycle_A: 0.680 idt_A: 0.439 D_B: 0.385 G_B: 0.661 cycle_B: 1.108 idt_B: 0.349 \n(epoch: 6, iters: 2000, time: 0.682, data: 0.002) D_A: 0.492 G_A: 0.134 cycle_A: 0.735 idt_A: 0.580 D_B: 0.172 G_B: 0.414 cycle_B: 1.266 idt_B: 0.381 \n(epoch: 6, iters: 2100, time: 0.683, data: 0.002) D_A: 0.242 G_A: 0.505 cycle_A: 1.231 idt_A: 0.599 D_B: 0.034 G_B: 0.835 cycle_B: 1.253 idt_B: 0.817 \n(epoch: 6, iters: 2200, time: 0.686, data: 0.002) D_A: 0.140 G_A: 0.491 cycle_A: 1.226 idt_A: 0.861 D_B: 0.044 G_B: 0.203 cycle_B: 1.675 idt_B: 0.342 \n(epoch: 6, iters: 2300, time: 0.928, data: 0.002) D_A: 0.297 G_A: 0.367 cycle_A: 0.547 idt_A: 0.527 D_B: 0.197 G_B: 0.236 cycle_B: 1.230 idt_B: 0.233 \n(epoch: 6, iters: 2400, time: 0.686, data: 0.002) D_A: 0.243 G_A: 0.201 cycle_A: 1.179 idt_A: 0.516 D_B: 0.177 G_B: 0.296 cycle_B: 0.996 idt_B: 0.445 \n(epoch: 6, iters: 2500, time: 0.680, data: 0.002) D_A: 0.292 G_A: 0.343 cycle_A: 0.827 idt_A: 0.574 D_B: 0.715 G_B: 0.652 cycle_B: 0.871 idt_B: 0.451 \nsaving the latest model (epoch 6, total_iters 15000)\nEnd of epoch 6 / 100 \t Time Taken: 1375 sec\nlearning rate 0.0002000 -> 0.0002000\n(epoch: 7, iters: 100, time: 0.684, data: 0.136) D_A: 0.149 G_A: 0.343 cycle_A: 1.205 idt_A: 0.542 D_B: 0.294 G_B: 0.641 cycle_B: 0.828 idt_B: 0.393 \n(epoch: 7, iters: 200, time: 1.140, data: 0.002) D_A: 0.150 G_A: 0.359 cycle_A: 0.959 idt_A: 0.249 D_B: 0.155 G_B: 0.493 cycle_B: 0.970 idt_B: 0.509 \n(epoch: 7, iters: 300, time: 0.687, data: 0.002) D_A: 0.207 G_A: 0.556 cycle_A: 0.699 idt_A: 0.685 D_B: 0.289 G_B: 0.155 cycle_B: 1.467 idt_B: 0.265 \n(epoch: 7, iters: 400, time: 0.684, data: 0.002) D_A: 0.311 G_A: 0.238 cycle_A: 1.071 idt_A: 0.465 D_B: 0.122 G_B: 0.410 cycle_B: 0.834 idt_B: 0.546 \n(epoch: 7, iters: 500, time: 0.681, data: 0.003) D_A: 0.149 G_A: 0.418 cycle_A: 0.463 idt_A: 0.325 D_B: 0.191 G_B: 0.276 cycle_B: 0.915 idt_B: 0.227 \n(epoch: 7, iters: 600, time: 0.929, data: 0.002) D_A: 0.310 G_A: 0.275 cycle_A: 1.057 idt_A: 0.458 D_B: 0.073 G_B: 0.616 cycle_B: 0.765 idt_B: 0.367 \n(epoch: 7, iters: 700, time: 0.684, data: 0.002) D_A: 0.057 G_A: 0.528 cycle_A: 0.804 idt_A: 0.500 D_B: 0.042 G_B: 0.661 cycle_B: 1.189 idt_B: 0.310 \n(epoch: 7, iters: 800, time: 0.682, data: 0.002) D_A: 0.084 G_A: 0.674 cycle_A: 1.317 idt_A: 0.521 D_B: 0.052 G_B: 1.154 cycle_B: 1.046 idt_B: 0.622 \n(epoch: 7, iters: 900, time: 0.682, data: 0.002) D_A: 0.186 G_A: 0.246 cycle_A: 1.245 idt_A: 0.782 D_B: 0.132 G_B: 0.696 cycle_B: 2.310 idt_B: 0.871 \n(epoch: 7, iters: 1000, time: 1.224, data: 0.002) D_A: 0.052 G_A: 0.861 cycle_A: 0.635 idt_A: 0.401 D_B: 0.102 G_B: 0.954 cycle_B: 0.859 idt_B: 0.172 \nsaving the latest model (epoch 7, total_iters 16000)\n(epoch: 7, iters: 1100, time: 0.684, data: 0.002) D_A: 0.398 G_A: 0.494 cycle_A: 1.221 idt_A: 0.601 D_B: 0.176 G_B: 0.092 cycle_B: 1.005 idt_B: 0.503 \n(epoch: 7, iters: 1200, time: 0.685, data: 0.002) D_A: 0.064 G_A: 0.981 cycle_A: 0.697 idt_A: 0.615 D_B: 0.131 G_B: 0.356 cycle_B: 0.830 idt_B: 0.355 \n(epoch: 7, iters: 1300, time: 0.686, data: 0.002) D_A: 0.212 G_A: 0.631 cycle_A: 1.034 idt_A: 0.550 D_B: 0.091 G_B: 0.467 cycle_B: 1.275 idt_B: 0.323 \n(epoch: 7, iters: 1400, time: 0.986, data: 0.002) D_A: 0.422 G_A: 0.468 cycle_A: 1.868 idt_A: 0.483 D_B: 0.395 G_B: 0.174 cycle_B: 0.882 idt_B: 0.817 \n(epoch: 7, iters: 1500, time: 0.681, data: 0.002) D_A: 0.154 G_A: 0.287 cycle_A: 2.382 idt_A: 0.249 D_B: 0.187 G_B: 0.417 cycle_B: 0.744 idt_B: 0.846 \n(epoch: 7, iters: 1600, time: 0.681, data: 0.002) D_A: 0.134 G_A: 0.498 cycle_A: 0.605 idt_A: 0.558 D_B: 0.189 G_B: 0.292 cycle_B: 1.259 idt_B: 0.208 \n(epoch: 7, iters: 1700, time: 0.683, data: 0.002) D_A: 0.186 G_A: 0.571 cycle_A: 0.863 idt_A: 1.706 D_B: 0.193 G_B: 0.207 cycle_B: 2.882 idt_B: 0.343 \n(epoch: 7, iters: 1800, time: 0.948, data: 0.002) D_A: 0.114 G_A: 0.572 cycle_A: 1.082 idt_A: 0.342 D_B: 0.158 G_B: 0.489 cycle_B: 0.953 idt_B: 0.565 \n(epoch: 7, iters: 1900, time: 0.686, data: 0.002) D_A: 0.176 G_A: 0.795 cycle_A: 1.419 idt_A: 0.325 D_B: 0.074 G_B: 0.498 cycle_B: 0.776 idt_B: 0.783 \n(epoch: 7, iters: 2000, time: 0.685, data: 0.002) D_A: 0.164 G_A: 0.171 cycle_A: 0.979 idt_A: 0.385 D_B: 0.341 G_B: 0.812 cycle_B: 1.288 idt_B: 0.489 \nsaving the latest model (epoch 7, total_iters 17000)\n(epoch: 7, iters: 2100, time: 0.686, data: 0.002) D_A: 0.148 G_A: 0.909 cycle_A: 0.889 idt_A: 1.515 D_B: 0.074 G_B: 0.568 cycle_B: 2.794 idt_B: 0.517 \n(epoch: 7, iters: 2200, time: 0.944, data: 0.002) D_A: 0.241 G_A: 0.709 cycle_A: 0.864 idt_A: 0.607 D_B: 0.324 G_B: 0.134 cycle_B: 1.685 idt_B: 0.267 \n(epoch: 7, iters: 2300, time: 0.683, data: 0.002) D_A: 0.265 G_A: 0.110 cycle_A: 1.161 idt_A: 0.300 D_B: 0.089 G_B: 0.284 cycle_B: 0.620 idt_B: 0.494 \n(epoch: 7, iters: 2400, time: 0.680, data: 0.002) D_A: 0.309 G_A: 0.194 cycle_A: 0.784 idt_A: 0.366 D_B: 0.323 G_B: 0.528 cycle_B: 0.847 idt_B: 0.343 \n(epoch: 7, iters: 2500, time: 0.683, data: 0.002) D_A: 0.075 G_A: 0.606 cycle_A: 0.858 idt_A: 0.330 D_B: 0.114 G_B: 0.686 cycle_B: 0.808 idt_B: 0.291 \nEnd of epoch 7 / 100 \t Time Taken: 1374 sec\nlearning rate 0.0002000 -> 0.0002000\n(epoch: 8, iters: 100, time: 1.235, data: 0.184) D_A: 0.414 G_A: 0.839 cycle_A: 0.914 idt_A: 0.336 D_B: 0.217 G_B: 0.150 cycle_B: 0.755 idt_B: 0.316 \n(epoch: 8, iters: 200, time: 0.683, data: 0.002) D_A: 0.078 G_A: 0.340 cycle_A: 0.712 idt_A: 0.302 D_B: 0.076 G_B: 0.680 cycle_B: 0.816 idt_B: 0.442 \n(epoch: 8, iters: 500, time: 1.185, data: 0.002) D_A: 0.107 G_A: 0.650 cycle_A: 0.755 idt_A: 0.785 D_B: 0.071 G_B: 0.525 cycle_B: 1.502 idt_B: 0.334 \nsaving the latest model (epoch 8, total_iters 18000)\n(epoch: 8, iters: 600, time: 0.685, data: 0.002) D_A: 0.105 G_A: 0.692 cycle_A: 0.694 idt_A: 0.317 D_B: 0.022 G_B: 0.728 cycle_B: 0.933 idt_B: 0.333 \n(epoch: 8, iters: 700, time: 0.686, data: 0.002) D_A: 0.250 G_A: 0.436 cycle_A: 0.883 idt_A: 0.456 D_B: 0.195 G_B: 0.389 cycle_B: 0.912 idt_B: 0.524 \n(epoch: 8, iters: 800, time: 0.686, data: 0.002) D_A: 0.080 G_A: 0.233 cycle_A: 1.009 idt_A: 0.855 D_B: 0.251 G_B: 0.700 cycle_B: 1.742 idt_B: 0.642 \n(epoch: 8, iters: 900, time: 0.951, data: 0.002) D_A: 0.472 G_A: 1.350 cycle_A: 1.587 idt_A: 0.419 D_B: 0.186 G_B: 0.185 cycle_B: 0.822 idt_B: 0.241 \n(epoch: 8, iters: 1000, time: 0.681, data: 0.002) D_A: 0.283 G_A: 0.022 cycle_A: 0.642 idt_A: 0.530 D_B: 0.349 G_B: 0.309 cycle_B: 1.320 idt_B: 0.228 \n(epoch: 8, iters: 1300, time: 0.943, data: 0.002) D_A: 0.289 G_A: 0.294 cycle_A: 0.784 idt_A: 0.499 D_B: 0.153 G_B: 0.263 cycle_B: 1.133 idt_B: 0.279 \n(epoch: 8, iters: 1400, time: 0.687, data: 0.002) D_A: 0.263 G_A: 0.339 cycle_A: 1.131 idt_A: 0.676 D_B: 0.390 G_B: 0.255 cycle_B: 1.002 idt_B: 0.391 \n(epoch: 8, iters: 1500, time: 0.686, data: 0.003) D_A: 0.352 G_A: 0.802 cycle_A: 1.935 idt_A: 0.392 D_B: 0.223 G_B: 0.149 cycle_B: 0.865 idt_B: 0.843 \nsaving the latest model (epoch 8, total_iters 19000)\n(epoch: 8, iters: 1600, time: 0.684, data: 0.002) D_A: 0.140 G_A: 0.298 cycle_A: 0.628 idt_A: 0.264 D_B: 0.143 G_B: 0.564 cycle_B: 0.804 idt_B: 0.267 \n(epoch: 8, iters: 1700, time: 0.882, data: 0.002) D_A: 0.113 G_A: 0.066 cycle_A: 1.186 idt_A: 0.620 D_B: 0.174 G_B: 0.377 cycle_B: 1.290 idt_B: 0.605 \n(epoch: 8, iters: 1800, time: 0.687, data: 0.002) D_A: 0.161 G_A: 0.250 cycle_A: 1.238 idt_A: 0.342 D_B: 0.368 G_B: 0.943 cycle_B: 1.027 idt_B: 0.585 \n(epoch: 8, iters: 1900, time: 0.683, data: 0.002) D_A: 0.171 G_A: 0.428 cycle_A: 0.883 idt_A: 1.071 D_B: 0.069 G_B: 0.462 cycle_B: 1.851 idt_B: 0.543 \n(epoch: 8, iters: 2000, time: 0.685, data: 0.002) D_A: 0.245 G_A: 0.101 cycle_A: 0.868 idt_A: 0.307 D_B: 0.283 G_B: 0.472 cycle_B: 1.113 idt_B: 0.440 \n(epoch: 8, iters: 2100, time: 0.930, data: 0.003) D_A: 0.174 G_A: 0.372 cycle_A: 1.141 idt_A: 0.398 D_B: 0.138 G_B: 0.441 cycle_B: 1.282 idt_B: 0.556 \n(epoch: 8, iters: 2200, time: 0.686, data: 0.002) D_A: 0.271 G_A: 0.554 cycle_A: 1.390 idt_A: 0.437 D_B: 0.182 G_B: 0.233 cycle_B: 0.924 idt_B: 0.335 \n(epoch: 8, iters: 2300, time: 0.686, data: 0.002) D_A: 0.320 G_A: 0.473 cycle_A: 1.026 idt_A: 0.509 D_B: 0.184 G_B: 0.266 cycle_B: 0.910 idt_B: 0.364 \n(epoch: 8, iters: 2400, time: 0.684, data: 0.002) D_A: 0.285 G_A: 0.875 cycle_A: 0.751 idt_A: 0.566 D_B: 0.372 G_B: 0.064 cycle_B: 1.144 idt_B: 0.293 \n(epoch: 8, iters: 2500, time: 1.200, data: 0.002) D_A: 0.142 G_A: 0.535 cycle_A: 0.804 idt_A: 0.305 D_B: 0.111 G_B: 0.357 cycle_B: 0.650 idt_B: 0.301 \nsaving the latest model (epoch 8, total_iters 20000)\nEnd of epoch 8 / 100 \t Time Taken: 1377 sec\nlearning rate 0.0002000 -> 0.0002000\n(epoch: 9, iters: 100, time: 0.683, data: 0.176) D_A: 0.223 G_A: 0.548 cycle_A: 0.597 idt_A: 0.469 D_B: 0.066 G_B: 0.467 cycle_B: 0.685 idt_B: 0.242 \n(epoch: 9, iters: 200, time: 0.687, data: 0.002) D_A: 0.243 G_A: 0.158 cycle_A: 0.970 idt_A: 0.680 D_B: 0.236 G_B: 0.360 cycle_B: 1.518 idt_B: 0.390 \n(epoch: 9, iters: 300, time: 0.685, data: 0.002) D_A: 0.254 G_A: 0.301 cycle_A: 0.931 idt_A: 0.685 D_B: 0.286 G_B: 0.114 cycle_B: 1.211 idt_B: 0.321 \n(epoch: 9, iters: 400, time: 1.171, data: 0.002) D_A: 0.224 G_A: 0.205 cycle_A: 0.795 idt_A: 0.418 D_B: 0.313 G_B: 0.316 cycle_B: 0.944 idt_B: 0.571 \n(epoch: 9, iters: 500, time: 0.678, data: 0.002) D_A: 0.243 G_A: 0.193 cycle_A: 0.861 idt_A: 0.406 D_B: 0.305 G_B: 0.587 cycle_B: 1.306 idt_B: 0.423 \n(epoch: 9, iters: 600, time: 0.686, data: 0.002) D_A: 0.170 G_A: 0.220 cycle_A: 1.195 idt_A: 0.409 D_B: 0.182 G_B: 0.653 cycle_B: 1.058 idt_B: 0.511 \n(epoch: 9, iters: 700, time: 0.682, data: 0.002) D_A: 0.286 G_A: 0.100 cycle_A: 0.828 idt_A: 0.806 D_B: 0.454 G_B: 0.653 cycle_B: 1.843 idt_B: 0.320 \n(epoch: 9, iters: 800, time: 0.945, data: 0.002) D_A: 0.170 G_A: 0.300 cycle_A: 0.689 idt_A: 0.331 D_B: 0.217 G_B: 0.351 cycle_B: 1.288 idt_B: 0.286 \n(epoch: 9, iters: 900, time: 0.690, data: 0.002) D_A: 0.213 G_A: 0.215 cycle_A: 1.092 idt_A: 0.506 D_B: 0.202 G_B: 0.336 cycle_B: 0.932 idt_B: 0.470 \n(epoch: 9, iters: 1000, time: 0.683, data: 0.002) D_A: 0.042 G_A: 0.620 cycle_A: 0.714 idt_A: 0.678 D_B: 0.112 G_B: 0.554 cycle_B: 1.244 idt_B: 0.269 \nsaving the latest model (epoch 9, total_iters 21000)\n(epoch: 9, iters: 1100, time: 0.684, data: 0.002) D_A: 0.107 G_A: 0.416 cycle_A: 1.063 idt_A: 0.658 D_B: 0.279 G_B: 1.028 cycle_B: 1.525 idt_B: 0.484 \n(epoch: 9, iters: 1200, time: 0.888, data: 0.003) D_A: 0.210 G_A: 0.342 cycle_A: 0.670 idt_A: 0.515 D_B: 0.324 G_B: 0.797 cycle_B: 0.657 idt_B: 0.277 \n(epoch: 9, iters: 1300, time: 0.685, data: 0.002) D_A: 0.169 G_A: 0.153 cycle_A: 0.872 idt_A: 0.306 D_B: 0.321 G_B: 0.963 cycle_B: 0.628 idt_B: 0.385 \n(epoch: 9, iters: 1600, time: 0.916, data: 0.002) D_A: 0.078 G_A: 0.096 cycle_A: 0.717 idt_A: 0.320 D_B: 0.250 G_B: 0.442 cycle_B: 0.871 idt_B: 0.251 \n(epoch: 9, iters: 1700, time: 0.686, data: 0.002) D_A: 0.250 G_A: 0.497 cycle_A: 0.676 idt_A: 0.523 D_B: 0.204 G_B: 0.470 cycle_B: 0.985 idt_B: 0.271 \n(epoch: 9, iters: 1800, time: 0.683, data: 0.002) D_A: 0.446 G_A: 0.050 cycle_A: 0.916 idt_A: 0.593 D_B: 0.211 G_B: 0.198 cycle_B: 0.806 idt_B: 0.498 \n(epoch: 9, iters: 1900, time: 0.681, data: 0.002) D_A: 0.064 G_A: 0.422 cycle_A: 2.177 idt_A: 0.424 D_B: 0.173 G_B: 0.498 cycle_B: 0.877 idt_B: 1.059 \n(epoch: 9, iters: 2000, time: 1.246, data: 0.002) D_A: 0.097 G_A: 0.174 cycle_A: 0.920 idt_A: 0.313 D_B: 0.317 G_B: 0.309 cycle_B: 0.865 idt_B: 0.470 \nsaving the latest model (epoch 9, total_iters 22000)\n(epoch: 9, iters: 2100, time: 0.686, data: 0.002) D_A: 0.101 G_A: 0.437 cycle_A: 0.858 idt_A: 0.353 D_B: 0.122 G_B: 0.829 cycle_B: 0.720 idt_B: 0.251 \n(epoch: 9, iters: 2200, time: 0.683, data: 0.002) D_A: 0.344 G_A: 0.426 cycle_A: 1.264 idt_A: 0.477 D_B: 0.084 G_B: 0.167 cycle_B: 1.060 idt_B: 0.474 \n(epoch: 9, iters: 2500, time: 0.690, data: 0.002) D_A: 0.297 G_A: 0.219 cycle_A: 0.684 idt_A: 0.576 D_B: 0.226 G_B: 0.226 cycle_B: 0.989 idt_B: 0.318 \nEnd of epoch 9 / 100 \t Time Taken: 1375 sec\nlearning rate 0.0002000 -> 0.0002000\n(epoch: 10, iters: 100, time: 0.685, data: 0.131) D_A: 0.108 G_A: 0.751 cycle_A: 0.721 idt_A: 0.522 D_B: 0.059 G_B: 0.365 cycle_B: 1.211 idt_B: 0.301 \n(epoch: 10, iters: 200, time: 0.686, data: 0.002) D_A: 0.156 G_A: 0.429 cycle_A: 1.133 idt_A: 0.401 D_B: 0.228 G_B: 0.256 cycle_B: 1.073 idt_B: 0.470 \n(epoch: 10, iters: 300, time: 1.240, data: 0.002) D_A: 0.095 G_A: 0.735 cycle_A: 1.096 idt_A: 0.330 D_B: 0.096 G_B: 0.603 cycle_B: 0.771 idt_B: 0.589 \n(epoch: 10, iters: 400, time: 0.688, data: 0.003) D_A: 0.145 G_A: 0.441 cycle_A: 0.929 idt_A: 0.627 D_B: 0.062 G_B: 0.910 cycle_B: 1.464 idt_B: 0.486 \n(epoch: 10, iters: 500, time: 0.687, data: 0.002) D_A: 0.141 G_A: 0.281 cycle_A: 0.761 idt_A: 0.357 D_B: 0.169 G_B: 0.358 cycle_B: 0.761 idt_B: 0.505 \nsaving the latest model (epoch 10, total_iters 23000)\n(epoch: 10, iters: 600, time: 0.684, data: 0.002) D_A: 0.104 G_A: 0.791 cycle_A: 0.748 idt_A: 0.673 D_B: 0.230 G_B: 0.298 cycle_B: 1.464 idt_B: 0.274 \n(epoch: 10, iters: 700, time: 0.993, data: 0.002) D_A: 0.116 G_A: 0.551 cycle_A: 0.619 idt_A: 0.323 D_B: 0.051 G_B: 0.588 cycle_B: 0.665 idt_B: 0.383 \n(epoch: 10, iters: 900, time: 0.683, data: 0.002) D_A: 0.231 G_A: 0.180 cycle_A: 1.322 idt_A: 0.405 D_B: 0.106 G_B: 0.245 cycle_B: 0.976 idt_B: 0.580 \n(epoch: 10, iters: 1000, time: 0.682, data: 0.002) D_A: 0.330 G_A: 0.317 cycle_A: 1.309 idt_A: 0.367 D_B: 0.233 G_B: 0.311 cycle_B: 0.767 idt_B: 0.645 \n(epoch: 10, iters: 1100, time: 0.942, data: 0.002) D_A: 0.145 G_A: 0.297 cycle_A: 0.739 idt_A: 0.286 D_B: 0.067 G_B: 0.394 cycle_B: 0.725 idt_B: 0.385 \n(epoch: 10, iters: 1200, time: 0.684, data: 0.002) D_A: 0.291 G_A: 0.138 cycle_A: 1.083 idt_A: 0.570 D_B: 0.204 G_B: 0.181 cycle_B: 0.800 idt_B: 0.492 \n(epoch: 10, iters: 1300, time: 0.682, data: 0.002) D_A: 0.063 G_A: 0.588 cycle_A: 0.904 idt_A: 0.212 D_B: 0.227 G_B: 0.174 cycle_B: 0.640 idt_B: 0.296 \n(epoch: 10, iters: 1400, time: 0.688, data: 0.002) D_A: 0.111 G_A: 0.247 cycle_A: 0.893 idt_A: 0.471 D_B: 0.090 G_B: 0.336 cycle_B: 0.978 idt_B: 0.191 \n(epoch: 10, iters: 1500, time: 1.168, data: 0.002) D_A: 0.050 G_A: 1.021 cycle_A: 0.540 idt_A: 0.229 D_B: 0.071 G_B: 0.548 cycle_B: 0.641 idt_B: 0.247 \nsaving the latest model (epoch 10, total_iters 24000)\n(epoch: 10, iters: 1600, time: 0.685, data: 0.002) D_A: 0.156 G_A: 0.401 cycle_A: 1.005 idt_A: 0.351 D_B: 0.161 G_B: 0.482 cycle_B: 1.186 idt_B: 0.548 \n(epoch: 10, iters: 1700, time: 0.683, data: 0.002) D_A: 0.158 G_A: 0.470 cycle_A: 0.579 idt_A: 0.442 D_B: 0.053 G_B: 0.128 cycle_B: 1.104 idt_B: 0.326 \n(epoch: 10, iters: 1800, time: 0.685, data: 0.002) D_A: 0.294 G_A: 0.218 cycle_A: 1.002 idt_A: 0.475 D_B: 0.195 G_B: 0.233 cycle_B: 1.148 idt_B: 0.491 \n(epoch: 10, iters: 1900, time: 0.954, data: 0.002) D_A: 0.296 G_A: 1.370 cycle_A: 0.889 idt_A: 0.594 D_B: 0.219 G_B: 0.248 cycle_B: 0.978 idt_B: 0.356 \n(epoch: 10, iters: 2200, time: 0.685, data: 0.002) D_A: 0.086 G_A: 0.470 cycle_A: 1.368 idt_A: 0.523 D_B: 0.148 G_B: 0.392 cycle_B: 1.076 idt_B: 0.650 \n(epoch: 10, iters: 2300, time: 0.943, data: 0.002) D_A: 0.021 G_A: 0.354 cycle_A: 0.832 idt_A: 0.347 D_B: 0.084 G_B: 0.685 cycle_B: 0.894 idt_B: 0.330 \n(epoch: 10, iters: 2400, time: 0.686, data: 0.002) D_A: 0.261 G_A: 0.231 cycle_A: 1.163 idt_A: 0.415 D_B: 0.516 G_B: 0.061 cycle_B: 0.882 idt_B: 0.397 \n(epoch: 10, iters: 2500, time: 0.684, data: 0.002) D_A: 0.056 G_A: 0.232 cycle_A: 0.585 idt_A: 0.541 D_B: 0.166 G_B: 0.375 cycle_B: 1.221 idt_B: 0.204 \nsaving the latest model (epoch 10, total_iters 25000)\nsaving the model at the end of epoch 10, iters 25000\nEnd of epoch 10 / 100 \t Time Taken: 1376 sec\nlearning rate 0.0002000 -> 0.0002000\n(epoch: 11, iters: 100, time: 0.686, data: 0.153) D_A: 0.127 G_A: 0.933 cycle_A: 1.457 idt_A: 0.412 D_B: 0.158 G_B: 0.643 cycle_B: 0.872 idt_B: 0.805 \n(epoch: 11, iters: 200, time: 1.276, data: 0.002) D_A: 0.171 G_A: 0.630 cycle_A: 0.737 idt_A: 0.312 D_B: 0.165 G_B: 0.515 cycle_B: 1.018 idt_B: 0.304 \n^C\nTraceback (most recent call last):\n  File \"/kaggle/working/pytorch-CycleGAN-and-pix2pix/train.py\", line 51, in <module>\n    model.set_input(data)         # unpack data from dataset and apply preprocessing\n    ^^^^^^^^^^^^^^^^^^^^^\n  File \"/kaggle/working/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py\", line 108, in set_input\n    self.real_A = input['A' if AtoB else 'B'].to(self.device)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!zip -r ghibli_finetune_checkpoints.zip ./checkpoints/ghibli_finetune/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T00:57:20.171600Z","iopub.execute_input":"2025-05-08T00:57:20.171914Z","iopub.status.idle":"2025-05-08T00:57:41.485007Z","shell.execute_reply.started":"2025-05-08T00:57:20.171878Z","shell.execute_reply":"2025-05-08T00:57:41.484330Z"}},"outputs":[{"name":"stdout","text":"  adding: checkpoints/ghibli_finetune/ (stored 0%)\n  adding: checkpoints/ghibli_finetune/latest_net_G_A.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/10_net_D_B.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/latest_net_D_A.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/loss_log.txt (deflated 81%)\n  adding: checkpoints/ghibli_finetune/10_net_G_B.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/latest_net_G.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/5_net_D_B.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/train_opt.txt (deflated 79%)\n  adding: checkpoints/ghibli_finetune/5_net_D_A.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/10_net_D_A.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/5_net_G_B.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/5_net_G_A.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/latest_net_G_B.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/web/ (stored 0%)\n  adding: checkpoints/ghibli_finetune/web/images/ (stored 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch010_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch009_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch002_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch007_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch008_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch004_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch006_real_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch007_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch011_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch003_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch003_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch001_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch002_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch008_real_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch009_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch004_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch009_real_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch010_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch006_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch007_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch010_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch010_real_A.png (deflated 1%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch010_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch003_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch001_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch009_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch004_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch009_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch001_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch002_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch008_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch001_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch006_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch003_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch008_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch011_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch005_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch010_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch003_real_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch002_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch001_real_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch004_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch005_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch011_real_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch005_real_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch004_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch002_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch002_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch007_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch006_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch004_real_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch005_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch005_fake_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch006_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch010_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch002_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch004_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch007_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch002_real_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch011_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch006_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch005_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch010_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch008_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch003_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch003_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch009_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch011_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch008_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch001_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch006_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch005_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch001_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch009_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch007_real_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch011_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch003_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch008_idt_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch011_idt_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch009_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch008_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch007_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch001_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch006_fake_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch007_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch005_real_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch011_rec_A.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/images/epoch004_rec_B.png (deflated 0%)\n  adding: checkpoints/ghibli_finetune/web/index.html (deflated 96%)\n  adding: checkpoints/ghibli_finetune/latest_net_D_B.pth (deflated 7%)\n  adding: checkpoints/ghibli_finetune/10_net_G_A.pth (deflated 7%)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### INFERENCE","metadata":{}},{"cell_type":"code","source":"!mkdir -p ./datasets/ghibli_test/testA\n!cp /kaggle/input/projecttest/Girl.png ./datasets/ghibli_test/testA/sample.jpg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T14:29:56.533834Z","iopub.execute_input":"2025-04-24T14:29:56.534105Z","iopub.status.idle":"2025-04-24T14:29:56.777922Z","shell.execute_reply.started":"2025-04-24T14:29:56.534082Z","shell.execute_reply":"2025-04-24T14:29:56.777121Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!mkdir -p ./checkpoints/ghibli_finetune/\n!cp /kaggle/input/modelweights/latest_net_*.pth ./checkpoints/ghibli_finetune/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T14:29:57.864806Z","iopub.execute_input":"2025-04-24T14:29:57.865064Z","iopub.status.idle":"2025-04-24T14:29:58.400026Z","shell.execute_reply.started":"2025-04-24T14:29:57.865042Z","shell.execute_reply":"2025-04-24T14:29:58.399198Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"!python test.py --dataroot ./datasets/ghibli_test --name ghibli_finetune --model test --no_dropout --direction AtoB","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T14:30:00.236939Z","iopub.execute_input":"2025-04-24T14:30:00.237252Z","iopub.status.idle":"2025-04-24T14:30:06.409666Z","shell.execute_reply.started":"2025-04-24T14:30:00.237224Z","shell.execute_reply":"2025-04-24T14:30:06.408991Z"}},"outputs":[{"name":"stdout","text":"----------------- Options ---------------\n             aspect_ratio: 1.0                           \n               batch_size: 1                             \n          checkpoints_dir: ./checkpoints                 \n                crop_size: 256                           \n                 dataroot: ./datasets/ghibli_test        \t[default: None]\n             dataset_mode: single                        \n                direction: AtoB                          \n          display_winsize: 256                           \n                    epoch: latest                        \n                     eval: False                         \n                  gpu_ids: 0                             \n                init_gain: 0.02                          \n                init_type: normal                        \n                 input_nc: 3                             \n                  isTrain: False                         \t[default: None]\n                load_iter: 0                             \t[default: 0]\n                load_size: 256                           \n         max_dataset_size: inf                           \n                    model: test                          \n             model_suffix:                               \n               n_layers_D: 3                             \n                     name: ghibli_finetune               \t[default: experiment_name]\n                      ndf: 64                            \n                     netD: basic                         \n                     netG: resnet_9blocks                \n                      ngf: 64                            \n               no_dropout: True                          \t[default: False]\n                  no_flip: False                         \n                     norm: instance                      \n                 num_test: 50                            \n              num_threads: 4                             \n                output_nc: 3                             \n                    phase: test                          \n               preprocess: resize_and_crop               \n              results_dir: ./results/                    \n           serial_batches: False                         \n                   suffix:                               \n                use_wandb: False                         \n                  verbose: False                         \n       wandb_project_name: CycleGAN-and-pix2pix          \n----------------- End -------------------\ndataset [SingleDataset] was created\ninitialize network with normal\nmodel [TestModel] was created\nloading the model from ./checkpoints/ghibli_finetune/latest_net_G.pth\n/kaggle/working/pytorch-CycleGAN-and-pix2pix/models/base_model.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(load_path, map_location=str(self.device))\n---------- Networks initialized -------------\n[Network G] Total number of parameters : 11.378 M\n-----------------------------------------------\ncreating web directory ./results/ghibli_finetune/test_latest\nprocessing (0000)-th image... ['./datasets/ghibli_test/testA/sample.jpg']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}